{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoCSOy7Gx8JO"
   },
   "source": [
    "# Semantic Segmentation - Automatic segmentation model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQircpmOyBlk"
   },
   "source": [
    "## Setup - Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVWFJ4I1QV2m"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import files as colab_files\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ra9zHaMayJwo"
   },
   "source": [
    "## Setup - Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwuJtEI3SMe9"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import threading\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVmYP48X_P9q"
   },
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "STRIDE = 64\n",
    "REMOVE_NOTHING_CHANCE = 75 # chance of removing a clear image\n",
    "# segnet | vgg_segnet | resnet50_segnet\n",
    "CURRENT_MODEL = \"segnet.resnet50_segnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rmx_Gypz3kWS"
   },
   "outputs": [],
   "source": [
    "WORK_DIR = \"/PATH/TO/DATA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNuID7mwxlpv"
   },
   "source": [
    "## Setup - Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqDLpCfvxlBA"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/divamgupta/image-segmentation-keras\n",
    "%cd image-segmentation-keras\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNp1gXJ1Srr5"
   },
   "outputs": [],
   "source": [
    "import keras_segmentation\n",
    "from keras_segmentation.models.model_utils import transfer_weights\n",
    "from keras_segmentation.pretrained import model_from_checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1Z3LpPR43fG"
   },
   "source": [
    "## Setup - Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3gFH5vb48w9"
   },
   "source": [
    "### Dataset related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tG2yBCQa-GVg"
   },
   "outputs": [],
   "source": [
    "# creates a new dataset folder and a dataset-gen.zip file\n",
    "def develop_dataset():\n",
    "    os.system(\"rm -rf dataset\")\n",
    "    os.system(\"unzip \\\"%s/dataset.zip\\\"\" % (WORK_DIR))\n",
    "    os.system(\"mkdir \\\n",
    "            dataset/train-x dataset/train-y \\\n",
    "            dataset/train-processed \\\n",
    "            out \\\n",
    "            tmp\")\n",
    "    convert_png()\n",
    "    cut_images(STRIDE) # clear_dataset() is already called in here\n",
    "    verify_data()\n",
    "    os.system(\"zip -0 -r dataset-gen.zip dataset/ && \\\n",
    "                mv dataset-gen.zip \\\"%s/\\\"\" % (WORK_DIR))\n",
    "\n",
    "# unzips the generated dataset folder from the dataset-gen zip file\n",
    "def create_dataset():\n",
    "    os.system(\"unzip \\\"%s/dataset-gen.zip\\\"\" % (WORK_DIR))\n",
    "    verify_data()\n",
    "\n",
    "# resets the dataset folder\n",
    "def reset_dataset():\n",
    "    os.system(\"rm -rf dataset\")\n",
    "    create_dataset()\n",
    "\n",
    "def verify_data():\n",
    "    train_x = 0 # train input files quantity\n",
    "    train_p = 0 # train processed files quantity\n",
    "    for file in glob.iglob(\"dataset/train-x/*.png\"):\n",
    "        train_x += 1\n",
    "    for file in glob.iglob(\"dataset/train-processed/*.png\"):\n",
    "        train_p += 1\n",
    "\n",
    "    print(\"[Verify data] train_x\", train_x, \", train_p\", train_p)\n",
    "    if train_x != train_p:\n",
    "        print(\"[ERROR] Dataset input and output quantity isn't synced\")\n",
    "\n",
    "# move the output folder\n",
    "def move_out(folder):\n",
    "    os.rename(\"out\", \"%s/%s\" % (WORK_DIR, folder))\n",
    "\n",
    "# converts all the dataset images from .tif to .png\n",
    "def convert_png():\n",
    "    for file in glob.iglob(\"dataset/*/**.tif\"):\n",
    "        img = cv2.imread(file, 0)\n",
    "        cv2.imwrite(file.replace(\".tif\", \".png\"), img)\n",
    "\n",
    "# clears the dataset (removes some images)\n",
    "def clear_dataset():\n",
    "    for file in glob.iglob(\"dataset/train-y/*.png\"):\n",
    "        img = cv2.imread(file, 0)\n",
    "        if np.sum(img) == 0:\n",
    "            if random.randint(0,100) <= REMOVE_NOTHING_CHANCE:\n",
    "                filename = file.split(\"/\")[-1]\n",
    "                file_x = \"dataset/train-x/\" + filename\n",
    "                file_y = file\n",
    "\n",
    "                os.system(\"rm \\\"%s\\\" \\\"%s\\\"\" % (file_x, file_y))\n",
    "\n",
    "# resets the out folder\n",
    "def reset_out():\n",
    "    if os.path.exists(\"out\"):\n",
    "        shutil.rmtree(\"out\")\n",
    "    if os.path.exists(\"tmp\"):\n",
    "        shutil.rmtree(\"tmp\")\n",
    "    os.mkdir(\"tmp\")\n",
    "    os.mkdir(\"out\")\n",
    "    os.mkdir(\"out/src\")\n",
    "    os.mkdir(\"out/split\")\n",
    "    os.mkdir(\"out/over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHNm8LXnAyuW"
   },
   "outputs": [],
   "source": [
    "# extends image by copying last pixel of each row/column\n",
    "def extend_image(file):\n",
    "    img = cv2.imread(file, 0)\n",
    "    height = len(img)\n",
    "    width = len(img[0])\n",
    "\n",
    "    new_height = math.ceil(height/SIZE) * SIZE\n",
    "    new_width = math.ceil(width/SIZE) * SIZE\n",
    "\n",
    "    img_out = np.zeros((new_height,new_width,1))\n",
    "    \n",
    "    # copies the image\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            img_out[y,x] = img[y,x]\n",
    "    \n",
    "    # expands the right area\n",
    "    for y in range(0, height):\n",
    "        for x in range(width, new_width):\n",
    "            img_out[y,x] = img[y,width-1]\n",
    "    \n",
    "    # expands the bottom area\n",
    "    for y in range(height, new_height):\n",
    "        for x in range(0, width):\n",
    "            img_out[y,x] = img[height-1,x]\n",
    "    \n",
    "    # expands the bottom-right area\n",
    "    for y in range(height, new_height):\n",
    "        for x in range(width, new_width):\n",
    "            img_out[y,x] = img[height-1,width-1]\n",
    "\n",
    "    cv2.imwrite(file, img_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdrRpsCJbsI5"
   },
   "outputs": [],
   "source": [
    "# doesn't download, just copies\n",
    "def download(folder, download_as=None):\n",
    "    if download_as is None:\n",
    "        download_as = folder\n",
    "    if \"/\" in download_as:\n",
    "        download_as = download_as.split(\"/\")[-1]\n",
    "\n",
    "    model_path_name = CURRENT_MODEL.replace(\".\",\"_\")\n",
    "    if not os.path.exists(\"%s/resultados_%s\" % (WORK_DIR, model_path_name)):\n",
    "        os.mkdir(\"%s/resultados_%s\" % (WORK_DIR, model_path_name))\n",
    "        \n",
    "    dest = \"%s/resultados_%s/%s\" % (WORK_DIR, model_path_name, download_as)\n",
    "    if os.path.exists(download_as):\n",
    "        shutil.rmtree(download_as)\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    \n",
    "    shutil.copytree(folder, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-vJoO-L4zr6"
   },
   "source": [
    "### Image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JMZkKzDqQLE"
   },
   "outputs": [],
   "source": [
    "# cut a single image and output its parts to 'outfolder' \n",
    "def cut_image(file, outfolder, stride=STRIDE):\n",
    "    filename = file.split(\"/\")[-1]\n",
    "    index = 0\n",
    "    img = cv2.imread(file, 0)\n",
    "    height = len(img)\n",
    "    width = len(img[0])\n",
    "    for y in range(0, height, stride):\n",
    "        for x in range(0, width, stride):\n",
    "            img_out = np.zeros((SIZE,SIZE,1))\n",
    "            for i in range(SIZE): # y\n",
    "                for j in range(SIZE): # x\n",
    "                    if i+y < height and j+x < width:\n",
    "                        img_out[i,j] = img[i+y,j+x]\n",
    "            cv2.imwrite(\"%s/%06d_%s\" % (outfolder, index, filename), img_out)\n",
    "            index += 1\n",
    "\n",
    "# returns a noised image\n",
    "def noise(image, var):\n",
    "    row,col,ch = image.shape\n",
    "    mean = 0\n",
    "    gauss = np.random.normal(mean,var,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = image + gauss\n",
    "\n",
    "    cv2.normalize(noisy, noisy, 0, 255, cv2.NORM_MINMAX, dtype=-1)\n",
    "    noisy = noisy.astype(np.uint8)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZUoO-pz5JTh"
   },
   "source": [
    "### Image core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1FaN0jPx2_-"
   },
   "outputs": [],
   "source": [
    "def cut_images(stride):\n",
    "    for file_x in glob.iglob(\"dataset/input/*.png\"):\n",
    "        filename = file_x.split(\"/\")[-1]\n",
    "        file_y = \"dataset/label/%s\" % (filename)\n",
    "\n",
    "        extend_image(file_x)\n",
    "        extend_image(file_y)\n",
    "\n",
    "        cut_image(file_x, \"dataset/train-x\")\n",
    "        cut_image(file_y, \"dataset/train-y\")\n",
    "    \n",
    "    for file_x in glob.iglob(\"dataset/test-input/*.png\"):\n",
    "        filename = file_x.split(\"/\")[-1]\n",
    "        file_y = \"dataset/test-label/%s\" % (filename)\n",
    "\n",
    "        extend_image(file_x)\n",
    "        extend_image(file_y)\n",
    "    \n",
    "    clear_dataset()\n",
    "    process_label()\n",
    "\n",
    "def process_label():\n",
    "    for file in glob.iglob(\"dataset/train-y/*.png\"):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        img = cv2.imread(file, 0)\n",
    "        for i in range(len(img)):\n",
    "            for j in range(len(img[i])):\n",
    "                if(img[i,j] > 0): img[i,j] = 1\n",
    "                else: img[i,j] = 0\n",
    "\n",
    "        cv2.imwrite(\"dataset/train-processed/\"+filename, img)\n",
    "\n",
    "# returns float iou score for a single test file\n",
    "def get_iou(filename):\n",
    "    file_out = \"out/src/out_%s\" % (filename)\n",
    "    file_target = \"dataset/test-label/%s\" % (filename)\n",
    "\n",
    "    if not os.path.isfile(file_out):\n",
    "        print(\"[Error/IOU Score] Out file not found\")\n",
    "        return -1\n",
    "    if not os.path.isfile(file_target):\n",
    "        print(\"[Error/IOU Score] Target file not found\")\n",
    "        return -1\n",
    "\n",
    "    out = cv2.imread(file_out, 0)\n",
    "    target = cv2.imread(file_target, 0)\n",
    "\n",
    "    # Evaluates the intersection \"area\"\n",
    "    intersect = 0\n",
    "    for i in range(len(target)):\n",
    "        for j in range(len(target[i])):\n",
    "            if(out[i,j] == 255 and target[i,j] == 255):\n",
    "                intersect += 1\n",
    "    \n",
    "    # Evaluates the union \"area\"\n",
    "    union = 0\n",
    "    for i in range(len(target)):\n",
    "        for j in range(len(target[i])):\n",
    "            if(out[i,j] == 255 or target[i,j] == 255):\n",
    "                union += 1\n",
    "\n",
    "    if union == 0:\n",
    "        union += 1\n",
    "        intersect += 1\n",
    "\n",
    "    iou_score = intersect/union\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNQ1xFLD5VQK"
   },
   "source": [
    "### Data Augmentation core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpbbK0VlVtVz"
   },
   "outputs": [],
   "source": [
    "# Niblack's technique to local binarization\n",
    "def niblack_bin(img, window_size):\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    for y in range(0, height-window_size, window_size):\n",
    "        for x in range(0, width-window_size, window_size):\n",
    "            max = -1\n",
    "            min = -1\n",
    "            arr = []\n",
    "            # scan\n",
    "            for sub_y in range(0, window_size):\n",
    "                for sub_x in range(0, window_size):\n",
    "                    if max == -1 or img[y+sub_y, x+sub_x] > max:\n",
    "                        max = img[y+sub_y, x+sub_x]\n",
    "                    if min == -1 or img[y+sub_y, x+sub_x] < min:\n",
    "                        min = img[y+sub_y, x+sub_x]\n",
    "                    arr.append(img[y+sub_y, x+sub_x])\n",
    "            local_std = np.std(np.asarray(arr))\n",
    "            mean = (int(max)+int(min))/2\n",
    "            local_thresh = mean + (-0.2)*local_std\n",
    "            # bin\n",
    "            for sub_y in range(0, window_size):\n",
    "                for sub_x in range(0, window_size):\n",
    "                    img[y+sub_y][x+sub_x] = 255 if img[y+sub_y][x+sub_x] > local_thresh else 0\n",
    "    return img\n",
    "\n",
    "# Auto-contrast\n",
    "def auto_contrast(img):\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    max = min = -1\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            if max == -1 or img[y, x] > max:\n",
    "                max = img[y, x]\n",
    "            if min == -1 or img[y, x] < min:\n",
    "                min = img[y, x]\n",
    "    mean = int(max)-int(min)\n",
    "    if mean == 0: mean = 1\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            a = img[y, x]\n",
    "            b = float(a-min)/mean*255\n",
    "            img[y, x] = b\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pouM76Sb5RUO"
   },
   "outputs": [],
   "source": [
    "# invert images\n",
    "def invert_images(folder):\n",
    "    for file in glob.iglob(\"%s/*\" % (folder)):\n",
    "        img = cv2.imread(file, 0)\n",
    "        img = cv2.bitwise_not(img)\n",
    "        cv2.imwrite(file, img)\n",
    "\n",
    "# opencv default normalization\n",
    "def normalize_images(folder):\n",
    "    for file in glob.iglob(\"%s/*\" % (folder)):\n",
    "        img = cv2.imread(file)\n",
    "        cv2.normalize(img, img, 0, 255, cv2.NORM_MINMAX, dtype=-1)\n",
    "        cv2.imwrite(file, img)\n",
    "\n",
    "# more light\n",
    "def whiten_images(folder):\n",
    "    for file in glob.iglob(\"%s/*\" % (folder)):\n",
    "        img = cv2.imread(file, 0)\n",
    "        blank = 15 * np.ones(shape=img.shape, dtype=np.uint8)\n",
    "        img = img + blank\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                if img[y, x] > 255:\n",
    "                    img[y, x] = 255\n",
    "        cv2.imwrite(file, img)\n",
    "\n",
    "# local binarization techniques (currently only Niblack's method)\n",
    "def local_bin(folder, window_size):\n",
    "    count = 0\n",
    "    ts = [None, None]\n",
    "    def run(file, window_size):\n",
    "        img = cv2.imread(file,0)\n",
    "        img = niblack_bin(img, window_size)\n",
    "        cv2.imwrite(file, img)\n",
    "    for file in glob.iglob(\"%s/*\" % (folder)):\n",
    "        if count == 2:\n",
    "            ts[0].join()\n",
    "            ts[1].join()\n",
    "            count = 0\n",
    "        ts[count] = threading.Thread(target=run, args=(file, window_size))\n",
    "        ts[count].start()\n",
    "        count += 1\n",
    "\n",
    "# auto contrast images\n",
    "def contrast_images(folder):\n",
    "    for file in glob.iglob(\"%s/*\" % (folder)):\n",
    "        img = cv2.imread(file,0)\n",
    "        img = auto_contrast(img)\n",
    "        cv2.imwrite(file, img)\n",
    "\n",
    "# merges images from folder_1 with the same from folder_2 and puts the output in the folder_1\n",
    "def merge_folder(folder_1, folder_2):\n",
    "    for file_img in glob.iglob(\"%s/*.png\" % (folder_1)):\n",
    "        print(file_img, file_img.replace(folder_1, folder_2))\n",
    "        img_1 = cv2.imread(file_img)\n",
    "        img_2 = cv2.imread(file_img.replace(folder_1, folder_2))\n",
    "        print(img_1.shape, img_2.shape)\n",
    "        img_out = cv2.addWeighted(img_1,1.0,img_2,0.2,0)\n",
    "        \n",
    "        cv2.imwrite(file_img, img_out)\n",
    "\n",
    "# histogram equalization (modification)\n",
    "def histogram_eq(folder):\n",
    "    for file in glob.iglob(\"%s/*\" % (folder)):\n",
    "        img = cv2.imread(file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.equalizeHist(img)\n",
    "        cv2.imwrite(file, img)\n",
    "\n",
    "# noise images (modification)\n",
    "def noise_images(folder, chance, var=5):\n",
    "    for file in glob.iglob(\"%s/*\" % (folder)):\n",
    "        if random.randint(0,100) > chance: continue\n",
    "        img = cv2.imread(file)\n",
    "        img = noise(img, var)\n",
    "        cv2.imwrite(file, img)\n",
    "\n",
    "# noise images (augmentation)\n",
    "def noise_aug_images(folder, chance, var=5):\n",
    "    for file_x in glob.iglob(\"dataset/train-x/*.png\"):\n",
    "        if random.randint(0,100) > chance: continue\n",
    "        file_y = file_x.replace(\"train-x\", \"train-processed\")\n",
    "        \n",
    "        img = cv2.imread(file_x)\n",
    "        img = noise(img, var)\n",
    "        cv2.imwrite(file_x.replace(\".png\", \"noise.png\"), img)\n",
    "\n",
    "        img = cv2.imread(file_y)\n",
    "        cv2.imwrite(file_y.replace(\".png\", \"noise.png\"), img)\n",
    "\n",
    "# flip images (augmentation)\n",
    "def flip_images(chance):\n",
    "    for file_x in glob.iglob(\"dataset/train-x/*.png\"):\n",
    "        if random.randint(0,100) > chance: continue\n",
    "        file_y = file_x.replace(\"train-x\", \"train-processed\")\n",
    "        \n",
    "        img = cv2.imread(file_x)\n",
    "        img = cv2.flip(img, 0)\n",
    "        cv2.imwrite(file_x.replace(\".png\", \"fp.png\"), img)\n",
    "\n",
    "        img = cv2.imread(file_y)\n",
    "        img = cv2.flip(img, 0)\n",
    "        cv2.imwrite(file_y.replace(\".png\", \"fp.png\"), img)\n",
    "        \n",
    "# rotate images (augmentation)\n",
    "def rotate_images(chance):\n",
    "    for file_x in glob.iglob(\"dataset/train-x/*.png\"):\n",
    "        if random.randint(0,100) > chance: continue\n",
    "        file_y = file_x.replace(\"train-x\", \"train-processed\")\n",
    "        \n",
    "        img = cv2.imread(file_x)\n",
    "        img = np.rot90(img)\n",
    "        img = np.rot90(img)\n",
    "        cv2.imwrite(file_x.replace(\".png\", \"fp.png\"), img)\n",
    "\n",
    "        img = cv2.imread(file_y)\n",
    "        img = np.rot90(img)\n",
    "        img = np.rot90(img)\n",
    "        cv2.imwrite(file_y.replace(\".png\", \"fp.png\"), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jofqi9Cr8zbj"
   },
   "source": [
    "## Setup - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndZWL0MC5ema"
   },
   "source": [
    "### Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sKcEJPJCTS1"
   },
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    reset_out()\n",
    "    \n",
    "    for file in glob.iglob(\"dataset/test-input/*.png\"):\n",
    "        cut_image(file, \"tmp\", stride=SIZE)\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        img_original = cv2.imread(file, 0)\n",
    "        break_width = math.ceil(len(img_original[0])/SIZE) # ceil because there's a padding at the left of the cropped image\n",
    "\n",
    "        img_in = None\n",
    "        img_in_x = None\n",
    "        img_out = None\n",
    "        img_out_x = None\n",
    "        current_width = 0\n",
    "        for file_in in sorted(glob.iglob(\"tmp/*\")):\n",
    "            model.predict_segmentation(\n",
    "                inp=file_in,\n",
    "                out_fname=\"tmp_out.png\"\n",
    "            )\n",
    "            temp_in = cv2.imread(file_in, 0)\n",
    "            temp_out = cv2.imread(\"tmp_out.png\", 0)\n",
    "\n",
    "            if img_out_x is None:\n",
    "                img_out_x = temp_out\n",
    "                img_in_x = temp_in\n",
    "            else:\n",
    "                img_out_x = np.append(img_out_x, temp_out, axis=1)\n",
    "                img_in_x = np.append(img_in_x, temp_in, axis=1)\n",
    "\n",
    "            current_width += 1\n",
    "\n",
    "            if current_width == break_width:\n",
    "                if img_out is None:\n",
    "                    img_out = img_out_x\n",
    "                    img_in = img_in_x\n",
    "                else:\n",
    "                    img_out = np.append(img_out, img_out_x, axis=0)\n",
    "                    img_in = np.append(img_in, img_in_x, axis=0)\n",
    "                img_out_x = None\n",
    "                img_in_x = None\n",
    "                current_width = 0\n",
    "        \n",
    "        for i in range(len(img_out)):\n",
    "            for j in range(len(img_out[i])):\n",
    "                if(img_out[i,j] > 200): img_out[i,j] = 255\n",
    "                else: img_out[i,j] = 0\n",
    "            \n",
    "        os.system(\"rm tmp/*\")\n",
    "\n",
    "        cv2.imwrite(\"out/src/out_%s\" % (filename), img_out)\n",
    "        cv2.imwrite(\"out/src/in_%s\" % (filename), img_in)\n",
    "\n",
    "        process_out(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kydu80eQH7pW"
   },
   "outputs": [],
   "source": [
    "def process_out(file):\n",
    "    img_in = cv2.imread(\"out/src/in_%s\" % (file), 1)\n",
    "    img_out = cv2.imread(\"out/src/out_%s\" % (file), 1)\n",
    "    img_target = cv2.imread(\"dataset/test-label/%s\" % (file), 1)\n",
    "    \n",
    "    # Gets the iou score\n",
    "    iou_score = get_iou(file)\n",
    "    if iou_score == -1:\n",
    "        print(\"Error while checking iou_score\")\n",
    "        return\n",
    "    \n",
    "    # Creates/appends the description file\n",
    "    create_header = False\n",
    "    if not os.path.isfile(\"out/description.csv\"):\n",
    "        create_header = True\n",
    "    with open(\"out/description.csv\", \"a\") as desc:\n",
    "        if create_header:\n",
    "            desc.write(\"name,iou\\n\")\n",
    "        desc.write(\"%s,%f\\n\" % (file, iou_score))\n",
    "\n",
    "    # Creates the \"splitscreen\" image\n",
    "    img = np.append(img_in, img_out, axis=1)\n",
    "    cv2.imwrite(\"out/split/%s\" % (file), img)\n",
    "\n",
    "    # Creates the overlayed image (in and out)\n",
    "    for i in range(len(img_out)):\n",
    "        for j in range(len(img_out[i])):\n",
    "            if img_out[i,j][0] == 255:\n",
    "                img_out[i,j] = [0, 0, 255] # B,G,R\n",
    "    img = cv2.addWeighted(img_in,1.0,img_out,0.2,0)\n",
    "    cv2.imwrite(\"out/%s\" % (file), img)\n",
    "\n",
    "    # Creates the overlayed image (in with out and target)\n",
    "    for i in range(len(img_target)):\n",
    "        for j in range(len(img_target[i])):\n",
    "            if img_target[i,j][0] == 255:\n",
    "                img_target[i,j] = [255, 0, 0] # B,G,R\n",
    "    img = cv2.addWeighted(img_in,1,img_out,0.5,0)\n",
    "    img = cv2.addWeighted(img,1,img_target,0.3,0)\n",
    "    cv2.imwrite(\"out/over/%s\" % (file), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKE2odZa82sE"
   },
   "outputs": [],
   "source": [
    "model = eval(\"keras_segmentation.models.\" + CURRENT_MODEL)(n_classes=2, input_height=SIZE, input_width=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhI-45eF8NGz"
   },
   "outputs": [],
   "source": [
    "def reset_all():\n",
    "    model = get_default_model()\n",
    "    reset_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWgvNl7W8g8c"
   },
   "outputs": [],
   "source": [
    "def log_out(texts):\n",
    "    with open(\"out/description.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EamrL3Gmv9Vl"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train( \n",
    "        train_images =  \"dataset/train-x/\",\n",
    "        train_annotations = \"dataset/train-processed/\",\n",
    "        checkpoints_path = \"/tmp/model-ckpt.h5\", epochs=15,\n",
    "        steps_per_epoch=1024\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVDSqTuZm5gV"
   },
   "outputs": [],
   "source": [
    "def fetch_model_results():\n",
    "    results_folder = \"%s/resultados_%s\" % (WORK_DIR, CURRENT_MODEL.replace(\".\",\"_\"))\n",
    "    model_file = \"%s/model.csv\" % (results_folder)\n",
    "\n",
    "    if not os.path.isfile(model_file):\n",
    "        with open(model_file, \"w\") as file:\n",
    "            file.write(\"name,iou,augmentations\\n\")\n",
    "\n",
    "\n",
    "    def get_file_lines(file_name):\n",
    "        lines = []\n",
    "        with open(file_name, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            lines[i] = lines[i].replace(\"\\n\",\"\")\n",
    "        return lines\n",
    "\n",
    "    for root, dirs, files in os.walk(results_folder, topdown = False):\n",
    "        for name in files:\n",
    "            if name == \"description.csv\":\n",
    "                path = os.path.join(root, name)\n",
    "                \n",
    "                csv_file = os.path.join(root, name)\n",
    "                txt_file = os.path.join(root, \"description.txt\")\n",
    "\n",
    "                # get the augmentations\n",
    "                txt_lines = get_file_lines(txt_file)\n",
    "                augmentations = \"/\".join(txt_lines)\n",
    "\n",
    "                # get the csv lines from training and append them with the augmentations\n",
    "                csv_lines = get_file_lines(csv_file)\n",
    "                csv_lines.pop(0)\n",
    "                for i in range(len(csv_lines)):\n",
    "                    csv_lines[i] = csv_lines[i] + \",\" + augmentations\n",
    "                \n",
    "                # write the results to the main model csv file\n",
    "                with open(model_file, \"a\") as file:\n",
    "                    for l in csv_lines:\n",
    "                        file.write(l + \"\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CGIDnqatHc_"
   },
   "outputs": [],
   "source": [
    "def get_default_model():\n",
    "    model = eval(\"keras_segmentation.models.\" + CURRENT_MODEL)(n_classes=2, input_height=SIZE, input_width=SIZE)\n",
    "    model.load_weights(\"%s/models/model-%s.h5.0\" % (WORK_DIR, CURRENT_MODEL.replace(\".\",\"_\")))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-x1PNk1kuj0P"
   },
   "outputs": [],
   "source": [
    "def create_default_model():\n",
    "    # Create default model\n",
    "    model = eval(\"keras_segmentation.models.\" + CURRENT_MODEL)(n_classes=2, input_height=SIZE, input_width=SIZE)\n",
    "    # Create blank dataset\n",
    "    img = np.zeros(shape=(SIZE,SIZE,1))\n",
    "    if not os.path.exists(\"blank_data_x/\"):\n",
    "        os.mkdir(\"blank_data_x\")\n",
    "    if not os.path.exists(\"blank_data_y/\"):\n",
    "        os.mkdir(\"blank_data_y\")\n",
    "    cv2.imwrite(\"blank_data_x/img0.png\", img)\n",
    "    cv2.imwrite(\"blank_data_y/img0.png\", img)\n",
    "    # Train default model\n",
    "    model.train( \n",
    "        train_images =  \"blank_data_x/\",\n",
    "        train_annotations = \"blank_data_y/\",\n",
    "        checkpoints_path = \"%s/models/model-%s.h5\" % (WORK_DIR, CURRENT_MODEL.replace(\".\",\"_\")),\n",
    "        epochs=1, steps_per_epoch=1\n",
    "    )\n",
    "    # Model is already saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLIkpiWu85Zs"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HV3-WKbM9Z_S"
   },
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6R_9anaW8-45"
   },
   "outputs": [],
   "source": [
    "create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_YrZj3Eumpt"
   },
   "outputs": [],
   "source": [
    "create_default_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44LQN-qJ9ban"
   },
   "source": [
    "### Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsmisemwzNYd"
   },
   "outputs": [],
   "source": [
    "model = get_default_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GI54jIDkk6jx"
   },
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrNSk2vvrf1B"
   },
   "outputs": [],
   "source": [
    "# standard\n",
    "reset_all()\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"limpo\"])\n",
    "download(\"out\", \"limpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6jLmKb38GWZ"
   },
   "outputs": [],
   "source": [
    "# standard\n",
    "reset_all()\n",
    "normalize_images(\"dataset/train-x\")\n",
    "normalize_images(\"dataset/test-input\")\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"normalizado\"])\n",
    "download(\"out\", \"normalizado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlQgXx5DalAa"
   },
   "outputs": [],
   "source": [
    "# standard\n",
    "reset_all()\n",
    "normalize_images(\"dataset/test-input\")\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"teste_normalizado\"])\n",
    "download(\"out\", \"teste normalizado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9Q3A3Y0arBq"
   },
   "outputs": [],
   "source": [
    "reset_all()\n",
    "local_bin(\"dataset/train-x\", window_size=16)\n",
    "local_bin(\"dataset/test-input\", window_size=16)\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"niblack bin k=-0.2\"])\n",
    "download(\"out\", \"niblack k=0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fySuO5n0ErYL"
   },
   "outputs": [],
   "source": [
    "reset_all()\n",
    "os.system(\"cp -r dataset/train-x dataset/train-x-bin\")\n",
    "os.system(\"cp -r dataset/test-input dataset/test-input-bin\")\n",
    "local_bin(\"dataset/train-x-bin\", window_size=16)\n",
    "local_bin(\"dataset/test-input-bin\", window_size=16)\n",
    "normalize_images(\"dataset/train-x\")\n",
    "normalize_images(\"dataset/test-input\")\n",
    "merge_folder(\"dataset/train-x\", \"dataset/train-x-bin\")\n",
    "merge_folder(\"dataset/test-input\", \"dataset/test-input-bin\")\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"merge\",\"norm\",\"niblack\"])\n",
    "download(\"out\", \"merge norm niblack k=0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnLAaRTlV7ZU"
   },
   "outputs": [],
   "source": [
    "reset_all()\n",
    "contrast_images(\"dataset/train-x\")\n",
    "contrast_images(\"dataset/test-input\")\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"auto contrast\"])\n",
    "download(\"out\", \"contrast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmu9PpOHccrc"
   },
   "outputs": [],
   "source": [
    "reset_all()\n",
    "histogram_eq(\"dataset/train-x\")\n",
    "histogram_eq(\"dataset/test-input\")\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"hist. eq.\"])\n",
    "download(\"out\", \"hist eq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzkaB8kvjKRg"
   },
   "outputs": [],
   "source": [
    "reset_all()\n",
    "normalize_images(\"dataset/train-x\")\n",
    "normalize_images(\"dataset/test-input\")\n",
    "noise_images(\"dataset/train-x\", 50, var=5)\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"normalizado\", \"ruido (aug) 50% σ²=5\"])\n",
    "download(\"out\", \"normalizado ruido-50-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzkaB8kvjKRg"
   },
   "outputs": [],
   "source": [
    "reset_all()\n",
    "normalize_images(\"dataset/train-x\")\n",
    "normalize_images(\"dataset/test-input\")\n",
    "noise_aug_images(\"dataset/train-x\", 50, var=5)\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"normalizado\", \"ruido (aug) 50% σ²=5\"])\n",
    "download(\"out\", \"normalizado ruidoaug-50-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--26wwa9iAIi"
   },
   "outputs": [],
   "source": [
    "reset_all()\n",
    "normalize_images(\"dataset/train-x\")\n",
    "normalize_images(\"dataset/test-input\")\n",
    "flip_images(50)\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"normalizado\", \"espelhado 50%\"])\n",
    "download(\"out\", \"normalizado espelhado-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sH41SLsmj2bJ"
   },
   "outputs": [],
   "source": [
    "reset_all()\n",
    "normalize_images(\"dataset/train-x\")\n",
    "normalize_images(\"dataset/test-input\")\n",
    "rotate_images(50)\n",
    "train()\n",
    "test_model()\n",
    "log_out([\"normalizado\", \"rotacionado 50%\"])\n",
    "download(\"out\", \"normalizado rotacionado-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nhkfd8zKyWCH"
   },
   "outputs": [],
   "source": [
    "fetch_model_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNTEMym_NQv9"
   },
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsCNkPGR26Ms"
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEWD-v5vfPOe"
   },
   "outputs": [],
   "source": [
    "develop_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfRuJginfQAz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segnet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
